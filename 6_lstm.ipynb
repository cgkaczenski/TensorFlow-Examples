{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    print(output.get_shape())\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "    #print(logits.get_shape().as_list())\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat_v2(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295927 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "wai wh ilc a si xvoxgmcppc sqjoolrg if rjthhetkvrmiivorbr  hvthztsrco ouzciwneey\n",
      "xzbp oem jidhn   r  yymaxrh avljkrafwoviajtorntiakqkvdkowwxsxqvbhfepfn  ihjr jtw\n",
      "e  fgiyz thmjzvrqb    raxnuee lliphutdkeridybwqvueoa mnassrf dbzrd mevrmxce lfth\n",
      "h c n hqiyo ksdeu vjqaoxjmekczg nmhn e vgr kg zwxre yb y o nh bs ibogi lrpyatfaf\n",
      "be ahpmu obcnregwfhjh ie dxt dgtg xbeml te gqsapp  dnmd yftbdliqhimtqso hht eykc\n",
      "================================================================================\n",
      "Validation set perplexity: 20.31\n",
      "Average loss at step 100: 2.582217 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.00\n",
      "Validation set perplexity: 10.46\n",
      "Average loss at step 200: 2.245723 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.43\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 300: 2.105216 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 400: 2.006566 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 500: 1.943604 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 600: 1.914951 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 700: 1.865611 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 800: 1.821045 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 900: 1.831829 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 1000: 1.829176 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "================================================================================\n",
      "x prenomy edable stalled bussely of meated comoral he enprise of the belize inva\n",
      "lert in seppored incried betheral wing to excerd crezbusials and fece engorny hu\n",
      "at severoly vinws in the sed the de conlenre main for co farse hevers and of acc\n",
      "mate phan an one nine nine two one nine zero noved westorn sarman creory cerent \n",
      "ber of subils on the spepide in nuwres peesiby of that it of maul c chice to win\n",
      "================================================================================\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1100: 1.779179 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1200: 1.753643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1300: 1.737696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1400: 1.749174 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1500: 1.740234 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1600: 1.748616 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1700: 1.713227 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1800: 1.673708 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1900: 1.649779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2000: 1.696315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "================================================================================\n",
      "bet i rigw moved for aiggod uch and s commonn revences feashiputs and norteany o\n",
      "goving dalpist on monly biald in pars to dow on consting zero have the gaect of \n",
      "aticslaust tendalographys to restitics calling of the sears to ficsiving whip de\n",
      " in mist ic origin a setri of workwame as peroducton it is experity won atchogn \n",
      "gendal the tritt rexiaglerian the slingian an the hower depportade agginute wote\n",
      "================================================================================\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2100: 1.686438 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2200: 1.681514 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2300: 1.641565 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2400: 1.661002 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2500: 1.679816 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2600: 1.658207 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2700: 1.657804 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2800: 1.653699 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2900: 1.650084 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3000: 1.649987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "yince south have a mujted mandriak which stare englewepking the passime in the v\n",
      "many magylly renexts and of states elesse phantitly which injop again formed sam\n",
      "pater as cousty pajaces orma or he aurse massbarsity of leade thus s sarched in \n",
      "d the regarsion by expersion of kassides and jefroy maonedy problen stamit weich\n",
      "e the the intertamed to pastion he relambosdendard be what are prequicted and al\n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3100: 1.631005 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3200: 1.646401 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3300: 1.634516 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3400: 1.669107 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3500: 1.656961 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3600: 1.671203 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3700: 1.648892 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3800: 1.644282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3900: 1.639656 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4000: 1.653875 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "pleph nigm hwirk usure one five opferational odyymed nough artain after word sof\n",
      " of their koran its of bost it cause putita where where and aacorzed masse friew\n",
      "bre saicol the game callaring of marridayolas and the chick be byntames the spec\n",
      "tile at reendentric get zorka secomical gener sleviry the orthogently pertoral r\n",
      " hadal birg a pocucisive of this walrory as honds shanom musitiak notel porkiod \n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4100: 1.632597 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4200: 1.635101 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4300: 1.615540 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4400: 1.608626 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4500: 1.615720 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4600: 1.613323 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4700: 1.623177 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4800: 1.632244 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4900: 1.633755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5000: 1.608286 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "================================================================================\n",
      "jeps see renningly and which concainal thraone to die cin an syltry actain of nt\n",
      "it pensuli miskinus as russman mack of massing yeng is a seculates dors cokp one\n",
      "romatear of and cold we bank the referse of by contains of could one nine one th\n",
      "ing solry with emblidd by cheme as autung jaya seadwresentand rudy and actified \n",
      "cedmentlely at just eghong take first vasite his seash shuspuntist for the compl\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5100: 1.604584 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5200: 1.591075 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5300: 1.578029 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5400: 1.580200 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5500: 1.566627 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5600: 1.578607 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5700: 1.566419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5800: 1.579857 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5900: 1.576750 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6000: 1.545296 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "s the usually s princ fingutories as a could preminater d mores was  embinive pr\n",
      "lies phromition to her grass the two frouns one eight nine two seeston bight bur\n",
      "galt of romes six other lideders hathers adder on the people three cetted in dem\n",
      "in cooned col was feathes origin direct of was altaishand generiph prevelady wes\n",
      "ives austruckeys thriters preses or musician the jegame aaphing this sam two whe\n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6100: 1.566538 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6200: 1.533221 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6300: 1.545562 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6400: 1.539068 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6500: 1.556100 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6600: 1.591029 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6700: 1.580536 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6800: 1.600713 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6900: 1.580202 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 7000: 1.576617 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "chang ach evest syftherval modelist attenles the black are somerainglanghting do\n",
      "m were hockifd rosegen the councild with the smook to calleds abong two zero thr\n",
      "dasbide enjostact spreient arabally its whas execut finish of was order d xingio\n",
      "e to masknes pildin suln one nine six sinies and for generally havinz which a co\n",
      "n knarchorded in syrmiitry s policibition end one zero zero one stised not the e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.29\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  0]\n",
      " [-1 13]]\n",
      "[[11  0]\n",
      " [-1 13]]\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    ix = tf.Variable([[2,3],[1,-5]])\n",
    "    iy = tf.Variable([[4,3],[1,-2]])\n",
    "    \n",
    "    x = tf.concat_v2([tf.concat_v2([ix, ix], 1), tf.concat_v2([ix, ix], 1)], 0)\n",
    "    y = tf.concat_v2([tf.concat_v2([iy, iy], 1), tf.concat_v2([iy, iy], 1)], 0)\n",
    "    \n",
    "    test = tf.matmul(ix, iy)\n",
    "    \n",
    "    z = tf.matmul(x,y)/2\n",
    "    iz = tf.slice(z, [2,2], [2,2])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "\n",
    "  print(session.run(test))\n",
    "  print(session.run(iz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  x = tf.Variable(tf.truncated_normal([vocabulary_size*2, num_nodes*2], -0.1, 0.1))\n",
    "  m = tf.Variable(tf.truncated_normal([num_nodes*2, num_nodes*2], -0.1, 0.1))\n",
    "  gate_b = tf.Variable(tf.zeros([1, num_nodes*2]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    i = tf.concat_v2([tf.concat_v2([i, i], 1), tf.concat_v2([i, i], 1)], 0)\n",
    "    o = tf.concat_v2([tf.concat_v2([o, o], 1), tf.concat_v2([o, o], 1)], 0)\n",
    "    \n",
    "    matrix = tf.sigmoid(tf.matmul(i, x)/2 + tf.matmul(o, m)/2 + gate_b)\n",
    "    shape = matrix.get_shape().as_list()\n",
    "    input_gate = tf.slice(matrix, [0,0], [shape[0]/2, shape[1]/2])\n",
    "    forget_gate = tf.slice(matrix, [0,shape[1]/2], [shape[0]/2, shape[1]/2])\n",
    "    update = tf.slice(matrix, [shape[0]/2,0], [shape[0]/2, shape[1]/2])\n",
    "    output_gate = tf.slice(matrix, [shape[0]/2,shape[1]/2], [shape[0]/2, shape[1]/2])\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat_v2(outputs, 0), w, b)\n",
    "    #print(logits.get_shape().as_list())\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat_v2(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.308489 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.34\n",
      "================================================================================\n",
      "gcxzqb  paov anz e faeyoh ku ci wyenjznaf tt zzt hoy emfiey t  i bub ceqfh etwe \n",
      "wstbxp upmegsuqbvt  e  mtd nsa  ohvtyhxetnmelxytsyhpzuilm a srzsoiccpcftlpgchnab\n",
      "c sv  coc    ijrn eetumfkgcoodstj hkcniiip storro    auk  mwvav ge gfxitaenrvche\n",
      "da dnzj a rk  jol egkab olwtqptqbdvtiu us tjwrebi h eetienxsxenvn iki jrq  dbrer\n",
      "ms rte u   akkilekhskaknlxwquiw niwtdut c shcffcdw dzotrqnlahv eakebtctrvlhuvvj \n",
      "================================================================================\n",
      "Validation set perplexity: 19.55\n",
      "Average loss at step 100: 2.857391 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.64\n",
      "Validation set perplexity: 17.28\n",
      "Average loss at step 200: 2.695939 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.06\n",
      "Validation set perplexity: 13.53\n",
      "Average loss at step 300: 2.527991 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.54\n",
      "Validation set perplexity: 11.70\n",
      "Average loss at step 400: 2.426369 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.70\n",
      "Validation set perplexity: 10.54\n",
      "Average loss at step 500: 2.391127 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.31\n",
      "Validation set perplexity: 10.92\n",
      "Average loss at step 600: 2.338503 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.55\n",
      "Validation set perplexity: 10.58\n",
      "Average loss at step 700: 2.290405 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.68\n",
      "Validation set perplexity: 9.90\n",
      "Average loss at step 800: 2.278304 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.14\n",
      "Validation set perplexity: 9.70\n",
      "Average loss at step 900: 2.256967 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.92\n",
      "Validation set perplexity: 9.75\n",
      "Average loss at step 1000: 2.221836 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.80\n",
      "================================================================================\n",
      "x oformo corstentios onider furizeschind osenus one the sylia an ingor foron the\n",
      "s verd serammoacterift lens aunestonsa the the ay the in ismer the traareds wen \n",
      "be sel zersomproridy the ers foushutise ive wicgorosithro scanger woumatilc geri\n",
      "es ofre kuldunevelatirsseterfsttirined krencistow averonter a rekpindy sstheruti\n",
      "kse skonerneaxivend one propdem of brotorp loumitcangersurired one zildshy mint \n",
      "================================================================================\n",
      "Validation set perplexity: 9.38\n",
      "Average loss at step 1100: 2.188286 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.15\n",
      "Validation set perplexity: 9.07\n",
      "Average loss at step 1200: 2.166625 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.36\n",
      "Validation set perplexity: 9.70\n",
      "Average loss at step 1300: 2.165144 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.87\n",
      "Validation set perplexity: 8.87\n",
      "Average loss at step 1400: 2.162094 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.23\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 1500: 2.149611 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.63\n",
      "Validation set perplexity: 8.64\n",
      "Average loss at step 1600: 2.127495 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.67\n",
      "Validation set perplexity: 9.23\n",
      "Average loss at step 1700: 2.088510 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.74\n",
      "Validation set perplexity: 9.04\n",
      "Average loss at step 1800: 2.064204 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.77\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 1900: 2.094489 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.35\n",
      "Validation set perplexity: 9.38\n",
      "Average loss at step 2000: 2.098961 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.81\n",
      "================================================================================\n",
      "licedre fyom peray wheve macted ate gased the eeped ine nif wich callie onereali\n",
      "k eebation exfial mestionspens haso tive of peeters sore repentrioga loel beed d\n",
      "bes mos eralle nine the fix caliny the spacand kricalin in medemale the gowy bic\n",
      "fyer figpel popeis to sutherys sere empeister goredipal in alde reive baritery o\n",
      "he divere seveno cactlumen palad seey poew arattesh enes habereer ofix rafiad fo\n",
      "================================================================================\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 2100: 2.089773 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.93\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 2200: 2.041790 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 2300: 2.053027 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "Validation set perplexity: 8.04\n",
      "Average loss at step 2400: 2.048926 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 2500: 2.040085 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.44\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 2600: 2.038269 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 2700: 2.022634 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 2800: 2.018608 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 2900: 2.006859 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 3000: 1.987658 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "================================================================================\n",
      "zeruet toowarl empotines b mere the gemerm fro acmer polite thees luld emualer i\n",
      "chiigmatter gereadice mamer gee asmer of for aty regmen the empolst and dif one \n",
      "lourd ver ylised creleapertity ted ws orver lettoriart am rearws reberngi hilem \n",
      "gerved spucrerperin trank sover its wrow derichsra hise furger a yromes eccheran\n",
      "once are lure feover rerederinf a thrie ware linoviles of aldlerd je ald slaciec\n",
      "================================================================================\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 3100: 1.994016 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 3200: 1.996216 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.94\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 3300: 2.018919 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 3400: 1.998649 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 3500: 2.004130 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 3600: 1.985839 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.41\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 3700: 1.962469 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 3800: 1.976161 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 3900: 1.976634 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 4000: 1.966613 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "================================================================================\n",
      "zer of hry fearly sictacd deyicions our lash rinth ortheritisia quirally keventr\n",
      "jesicept und in simistdy tor pordanguent calr pephirany ferurited viminory niven\n",
      "feppipled brome ablo by it has wirctlaty bytronows wis misize penydasi the verio\n",
      "re pririce carmatq iistha phirite wit ypodinisent where the wight tre op leated \n",
      "aric tentreting one sever of fir dlonlaid lave tolme hyse entry and caur mertant\n",
      "================================================================================\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 4100: 1.962794 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 4200: 1.949342 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 4300: 1.946089 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 4400: 1.929654 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.34\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 4500: 1.928888 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 4600: 1.954168 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 4700: 1.938041 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 4800: 1.951428 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.11\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 4900: 1.903792 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 5000: 1.928095 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "================================================================================\n",
      " senges six gebatu gorstory eversions vomel toke biblf the seities fram sever se\n",
      "ge in two zero one two cill lis escushimen sirst beaut were and is the to fove i\n",
      "racs ale are recuses a trowenfive ourd the stater cares feven eight insthore oc \n",
      "vers as mecuil gocelworin was kight copred to five mocornusnoly comical of berra\n",
      "wer a condentate the esemers os six cored mortative fix picomst on perenfly e ro\n",
      "================================================================================\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5100: 1.903963 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 5200: 1.876908 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 5300: 1.880185 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 5400: 1.881263 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 5500: 1.887329 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 5600: 1.879909 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 5700: 1.883392 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 5800: 1.886897 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 5900: 1.854433 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 6000: 1.865692 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.76\n",
      "================================================================================\n",
      "kan to monig crefuctluse a hism no hievee the x the s purobries commatia tan un \n",
      "pementry to deqssich leeal troighally bast fefem prodwic of the stort woan nine \n",
      "bel in extrofice scelly be knagingal if the one lajoon two a rear chow pand from\n",
      "ng he thercured accament not name efst byore begapic mates a toptutes is wooll i\n",
      "perlig wher chomor amild lawe thill beal ejela allal feanider the asthay firze e\n",
      "================================================================================\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 6100: 1.845618 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 6200: 1.855201 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 6300: 1.841494 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 6400: 1.865169 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 6500: 1.891455 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 6600: 1.878923 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 6700: 1.903468 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 6800: 1.876613 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 6900: 1.883072 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 7000: 1.859720 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.73\n",
      "================================================================================\n",
      "neares band lewss us an ly the nine eight popiter mannaki five and pectert pymin\n",
      "qodime ortently in the the uniont oral cmpian erchurnay vist for whers he nivein\n",
      "sicc five courd teny is twoco to the rosumation dechohkmenton two stotternies an\n",
      "zer spelding usican were th the lay to chain two eighh age cocownen a loulita le\n",
      "festifen altidancredry refipars see exeage one fe on vith in one seven nine two \n",
      "================================================================================\n",
      "Validation set perplexity: 6.75\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
